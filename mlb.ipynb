{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-combination",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1. Module Installation & Data Loading\n",
    "# 1-1. Module Installation\n",
    "########################################\n",
    "import importlib.util\n",
    "import sys\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "# Required Modules\n",
    "MODULES = ['pandas', 'nltk', 'gensim', 'matplotlib', 'pyLDAvis']\n",
    "\n",
    "# Install Modules (if any of them are not installed yet)\n",
    "def install_modules(modules=MODULES):\n",
    "    for module in modules:\n",
    "        if module not in sys.modules and importlib.util.find_spec(module) is None:\n",
    "            !{sys.executable} -m pip install -q {module} --upgrade\n",
    "\n",
    "# Module Installation\n",
    "install_modules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-browser",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 1-2. Setup & Data Loading\n",
    "########################################\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Seed for Reproducible Results\n",
    "SEED = 2\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "\n",
    "DATA_PATH = 'Data' # Data Folder's Name\n",
    "assert os.path.isdir(DATA_PATH)\n",
    "ARTICLES_FILE = 'articles.csv' # Data Processed Dataset\n",
    "MVPS_FILE = 'mvps.csv' # Supporting Dataset\n",
    "\n",
    "# Configure Years & Months\n",
    "MIN_MAX_MONTHS = {'min': 4, 'max': 10} # Range of Months for the Articles\n",
    "MIN_MAX_YEARS = {'min': 2011, 'max': 2021} # Range of Years for the Articles\n",
    "YEARS = [2018, 2019, 2020, 2021] # Range of Years to Evaluate\n",
    "\n",
    "# Specify Columns & Data Types\n",
    "USECOLS = ['id', 'headline', 'summary', 'created', 'source']\n",
    "DTYPES = {'id': str, 'headline': str, 'summary': str, 'created': str, 'source': str}\n",
    "COLUMNS = ['id', 'year', 'text']\n",
    "\n",
    "# Specify Columns & Data Types for Supporting Dataset\n",
    "USECOLS_SUP = ['id', 'name', 'team', 'league', 'year']\n",
    "DTYPES_SUP = {'id': int, 'name': str, 'team': str, 'league': str, 'year': int}\n",
    "\n",
    "# Load Dataframe\n",
    "def load_dataframe(file_path, usecols, dtype):\n",
    "    df = pd.read_csv(file_path, usecols=usecols, dtype=dtype)\n",
    "    return df\n",
    "\n",
    "# Concat Csvs\n",
    "def concat_csvs():\n",
    "    # Load Dataset in Data Folder (reuters.csv, mlb.csv, wsj.csv, nyt.csv, espn.csv)\n",
    "    csvs = []\n",
    "    for [r, d, f] in os.walk(DATA_PATH): # r=root directory path, d=directory names, f=file names\n",
    "        for file in f:\n",
    "            if file not in [ARTICLES_FILE, MVPS_FILE]:\n",
    "                csv = load_dataframe(('%s/%s' % (DATA_PATH, file)), USECOLS, DTYPES)\n",
    "                csvs.append(csv)\n",
    "    articles = pd.concat(csvs)\n",
    "    return articles\n",
    "\n",
    "# Articles Dataset\n",
    "articles = concat_csvs()\n",
    "\n",
    "# MVPs (Supporting) Dataset\n",
    "mvps = load_dataframe(('%s/%s' % (DATA_PATH, MVPS_FILE)), USECOLS_SUP, DTYPES_SUP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-region",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 2. Data Cleaning & Preprocessing\n",
    "########################################\n",
    "import calendar\n",
    "import string\n",
    "import re\n",
    "import csv\n",
    "\n",
    "# Filter Created\n",
    "def filter_created(df1):\n",
    "    dfs = []\n",
    "    for year in range(MIN_MAX_YEARS['min'], MIN_MAX_YEARS['max']+1):\n",
    "        if year == MIN_MAX_YEARS['max'] and end_month < start_month:\n",
    "            break\n",
    "        \n",
    "        # Start Date\n",
    "        start_month = MIN_MAX_MONTHS['min']\n",
    "        start_month = ('0' if start_month < 10 else '') + str(start_month)\n",
    "        start_date = pd.to_datetime(str(year) + start_month + '01')\n",
    "        \n",
    "        # End Date\n",
    "        end_month = MIN_MAX_MONTHS['max']\n",
    "        if end_month < MIN_MAX_MONTHS['min']:\n",
    "            year += 1\n",
    "        end_day = calendar.monthrange(year, end_month)[1] # number of days in the month\n",
    "        end_day = ('0' if end_day < 10 else '') + str(end_day)\n",
    "        end_month = ('0' if end_month < 10 else '') + str(end_month)\n",
    "        end_date = pd.to_datetime(str(year) + end_month + end_day)\n",
    "        \n",
    "        df_ = df1[(pd.to_datetime(df1['created']) >= start_date) & (pd.to_datetime(df1['created']) <= end_date)]\n",
    "        dfs.append(df_)\n",
    "    df = pd.concat(dfs)\n",
    "    return df\n",
    "\n",
    "# Remove Duplicates\n",
    "def remove_duplicates(df1):\n",
    "    df = df1.drop_duplicates(subset=['headline', 'summary'])\n",
    "    return df\n",
    "\n",
    "# Preprocess Text\n",
    "def preprocess_text(text):\n",
    "    text = text.str.lower() # lowercase\n",
    "    text = text.apply(lambda x: re.sub(r'\\n', ' ', str(x))) # replace newline with space\n",
    "    text = text.apply(lambda x: re.sub(r'\\r', ' ', str(x))) # replace return with space\n",
    "    \n",
    "    # Replace punctuation (special characters) with a whitespace\n",
    "    dictionary = dict.fromkeys(string.punctuation, ' ') # e.g. {'=' : ' '}\n",
    "    trans_dictionary = str.maketrans(dictionary) # dictionary for translation \n",
    "    text = text.str.translate(trans_dictionary) # replace punctuation with a whitespace\n",
    "    \n",
    "    text = text.apply(lambda x: re.sub(r'\\d+', ' ', str(x))) # replace digit with a whitespace\n",
    "    text = text.apply(lambda x: re.sub(r'\\ss\\s', ' ', str(x))) # replace (whitespace)s(whitespace) with a whitespaceace\n",
    "    text = text.apply(lambda x: re.sub(r'\\s+', ' ', str(x))) # replace multiple whitespaces with a whitespace\n",
    "    text = text.apply(lambda x: re.sub(r'\\s(\\w{1})\\s(\\w{1})\\s',  r' \\1\\2 ', str(x))) # e.g. ' n y ' -> 'ny'\n",
    "    text = text.apply(lambda x: re.sub(r'(^\\w{1})\\s(\\w{1})\\s',  r'\\1\\2 ', str(x))) # e.g. 'n y ' -> 'ny'\n",
    "    text = text.apply(lambda x: re.sub(r'(\\w+)\\s([t]{1})\\s',  r'\\1\\2 ', str(x))) # e.g. 'wasn t ' -> 'wasnt'\n",
    "    text = text.str.strip() # strip the left and the right whitespace\n",
    "    return text\n",
    "\n",
    "# Concat Text (Headline and Summary)\n",
    "def concat_text(df1):\n",
    "    df1 = df1.sort_values(['created'], ascending=True)\n",
    "    df = pd.DataFrame(columns=['id', 'year', 'text'])\n",
    "    for row in df1.itertuples():\n",
    "        df = df.append({'id': row.id,\n",
    "                        'year': int(row.created[:4]),\n",
    "                        'text': str(' '.join([str(row.headline), str(row.summary)]))}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Concat Name\n",
    "def concat_name(df2):\n",
    "    df2 = df2.sort_values(['year'], ascending=True)\n",
    "    df = pd.DataFrame(columns=['year', 'name'])\n",
    "    for y in range(MIN_MAX_YEARS['min'], MIN_MAX_YEARS['max']+1):\n",
    "        rows = df2[df2['year'].astype(int)==y]\n",
    "        df = df.append({'year': int(y), 'name': str(';'.join(rows['name'].str.lower()))}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Merge Articles (& MVPs as option)\n",
    "def save_articles(df1, df2=None, file_name=ARTICLES_FILE):\n",
    "    if mvps is None:\n",
    "        df1 = pd.merge(df1, df2, how='left', on=['year'])\n",
    "        COLUMNS.append('name')\n",
    "    df = df1.sort_values(['id'], ascending=True)\n",
    "    df.to_csv('%s/%s' % (DATA_PATH, file_name), columns=COLUMNS,\n",
    "              index=False, header=True, quoting=csv.QUOTE_NONE, escapechar='')\n",
    "    return df\n",
    "\n",
    "# Preprocess Articles (& MVPs as option)\n",
    "def preprocess_articles(df1, df2=None):\n",
    "    df1 = filter_created(df1)\n",
    "    df1 = remove_duplicates(df1)\n",
    "    df1.headline = preprocess_text(df1.headline)\n",
    "    df1.summary = preprocess_text(df1.summary)\n",
    "    df1 = concat_text(df1)\n",
    "    if df2 is not None:\n",
    "        df2 = concat_name(df2)\n",
    "    df = save_articles(df1, df2) # COLUMNS=['id', 'year', 'text', 'name']\n",
    "    return df\n",
    "\n",
    "# Preprocessed Article (& MVPs as option)\n",
    "articles = preprocess_articles(articles, mvps)\n",
    "\n",
    "# Stats: Number of Articles per Year\n",
    "articles.groupby(['year']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-gibraltar",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3. Document Processing\n",
    "########################################\n",
    "import nltk\n",
    "nltk.download('wordnet') # for lemmatizer\n",
    "import gensim\n",
    "STOP_WORDS = ['mlb', 'major', 'league', 'baseball', 'game', 'team', 'player']\n",
    "stop_words = gensim.parsing.preprocessing.STOPWORDS.union(set(STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3-1. Lemmatizer & Stemmer Test\n",
    "# Find out which method to use\n",
    "########################################\n",
    "pd.set_option('display.max_columns', None) # None=auto detect\n",
    "\n",
    "def test_lemmatizer_stemmer(text):\n",
    "    # Stop Words Removal\n",
    "    swr = [token for token in text if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    lemmatized = ['Lemmatizer'] + [lemmatizer.lemmatize(token) for token in swr]\n",
    "    \n",
    "    # Snowball English Stemmer\n",
    "    snowball = nltk.stem.snowball.EnglishStemmer()\n",
    "    snowballed = ['Snowball'] + [snowball.stem(token) for token in swr]\n",
    "    \n",
    "    # Porter Stemmer\n",
    "    porter = nltk.stem.porter.PorterStemmer()\n",
    "    portered = ['Porter'] + [porter.stem(token) for token in swr]\n",
    "    \n",
    "    # Lancaster Stemmer\n",
    "    lancaster = nltk.stem.lancaster.LancasterStemmer()\n",
    "    lancastered = ['Lancaster'] + [lancaster.stem(token) for token in swr]\n",
    "    \n",
    "    df = pd.DataFrame([lemmatized, snowballed, portered, lancastered], columns=[''] + swr)\n",
    "    return df\n",
    "\n",
    "def print_text_note(text, note):\n",
    "    print('Sample Text:\\n', ' '.join(text), '\\n' + note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a65a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3-1-1. Lemmatizer & Stemmer Test1\n",
    "# Find out Lemmatizer works fine\n",
    "########################################\n",
    "sample_text1 = articles.text.values[101].split()\n",
    "note1 = \"NOTE: Lemmatizer groups words, Porter transforms 'los' into 'lo' and Lancaster transforms 'dodgers' into 'dodg.'\"\n",
    "print_text_note(sample_text1, note1)\n",
    "test_lemmatizer_stemmer(sample_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa08e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3-1-2. lemmatizer & Stemmer Test2\n",
    "# Find out Snowball Stemmer works fine\n",
    "########################################\n",
    "sample_text2 = 'dodging dodgers bankruptcies testimonies committed commits commissioner commissioning laud laudly generous generically'.split()\n",
    "note2 = \"NOTE: Snowball treats 'laud' & 'laudly' same, 'generous' and 'generically' differently. Snowball seems the best.\"\n",
    "print_text_note(sample_text2, note2)\n",
    "test_lemmatizer_stemmer(sample_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb2381",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3-2. Vocabulary\n",
    "########################################\n",
    "# Minimum total count in the collection\n",
    "MIN_COUNT = 1 \n",
    "\n",
    "# Phrase of words `a` and `b`: (cnt(a, b) - min_count) * TotalVocabSize / (cnt(a) * cnt(b)) > threshold\n",
    "THRESHOLD = 1 # smaller: longer phrases (e.g. Los Angeles Angeles)\n",
    "    \n",
    "# Create Vocabulary\n",
    "def create_vocabulary(docs):\n",
    "    # Tokenize Words\n",
    "    docs_tokenized = [doc.split() for doc in docs]\n",
    "    \n",
    "    # Lemmatize Words\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    docs_lemmatized = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs_tokenized]\n",
    "    \n",
    "    # Remove Stop-Words\n",
    "    docs_sw_removed = [[token for token in doc if token not in stop_words] for doc in docs_lemmatized]\n",
    "\n",
    "    # Apply Snowball English Stemming\n",
    "    stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "    docs_stemmed = [[stemmer.stem(token) for token in doc] for doc in docs_sw_removed]\n",
    "    return docs_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef650559",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3-3. MIN_COUNT Test\n",
    "# Find out the value for MIN_COUNT\n",
    "########################################\n",
    "def print_vocab_note(docs, note):\n",
    "    vocab = create_vocabulary(docs)\n",
    "    print('Sample Processed Documents:\\n', vocab, '\\n' + note)\n",
    "    return vocab\n",
    "\n",
    "def test_min_count(docs, min_count):\n",
    "    print('\\n' + 'MIN_COUNT = ' + str(min_count))\n",
    "    bigram = gensim.models.Phrases(docs, min_count=min_count, threshold=1).freeze()\n",
    "    for doc in docs:\n",
    "        print(bigram[doc])\n",
    "\n",
    "sample_docs = [\"He plays baseball in Los Angeles today\", \"He is playing in Los Angeles today\", \"He played with Angeles today\"]\n",
    "note = \"NOTE: There are 3 'angel today' and 2 'los angel'. It is hard to configure MIN_COUNT. MIN_COUNT = 1 seems the best due to 'los_angel'.\"\n",
    "sample_vocab = print_vocab_note(sample_docs, note)\n",
    "\n",
    "test_min_count(sample_vocab, 1) # MIN_COUNT = 1\n",
    "test_min_count(sample_vocab, 2) # MIN_COUNT = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bb1076",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# 3-4. N-gram Model & Terms\n",
    "########################################\n",
    "# Create N-gram Model\n",
    "def create_ngram_model(vocab):\n",
    "    ngram_model = gensim.models.Phrases(vocab, min_count=MIN_COUNT, threshold=THRESHOLD).freeze()\n",
    "    return ngram_model\n",
    "\n",
    "# Create Trigram Terms\n",
    "def create_trigram_terms(docs):\n",
    "    # Vocabulary\n",
    "    vocab = create_vocabulary(docs)\n",
    "    \n",
    "    bigram_model = create_ngram_model(vocab)\n",
    "    bigram_terms = [bigram_model[v] for v in vocab]\n",
    "    trigram_model = create_ngram_model(bigram_model[vocab])\n",
    "    trigram_terms = [trigram_model[terms] for terms in bigram_terms]\n",
    "    return trigram_terms\n",
    "\n",
    "########################################\n",
    "# 3-5. Dictionary & Corpus\n",
    "########################################\n",
    "# Create Dictionary: {TermID: Term} (e.g. {0: 'apple', 1: 'banana'})\n",
    "def create_dictionary(trigram_terms):\n",
    "    # Dictionary\n",
    "    dictionary = gensim.corpora.Dictionary(trigram_terms)\n",
    "    return dictionary\n",
    "\n",
    "# Trigram Terms for Dictionary\n",
    "trigram_terms_dict = create_trigram_terms(articles.text.values)\n",
    "\n",
    "# Create Dictionary for LDA Models\n",
    "DICTIONARY = create_dictionary(trigram_terms_dict)\n",
    "\n",
    "# Create Corpus: [(TermID, Frequency)] (e.g. [(0, 1), (1, 2), ...])\n",
    "def create_corpus(trigram_terms, dictionary=DICTIONARY):\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in trigram_terms]\n",
    "    return corpus\n",
    "\n",
    "# Create Trigram Terms & Corpus\n",
    "def create_trigram_corpus(docs):    \n",
    "    # Trigram Terms\n",
    "    trigram_terms = create_trigram_terms(docs)\n",
    "    \n",
    "    # Corpus\n",
    "    corpus = create_corpus(trigram_terms)\n",
    "    return trigram_terms, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 4. Topic Model & Coherence Score\n",
    "###################################\n",
    "# LDA Model\n",
    "NUMS_TOPICS = {'min': 2, 'max': 10} # Range of the Numbers of Topics for a Topic Model \n",
    "NUM_TOPICS = 5 # Default Number of Topics in the Corpus\n",
    "CHUNKSIZE = 100 # Number of Documents in each Chunk\n",
    "PASSES = 10 # Number of Passes in the Corpus\n",
    "RANDOM_STATE = SEED # Seed for Reproducibility\n",
    "\n",
    "# Build LDA Model\n",
    "def build_lda_model(corpus, dictionary=DICTIONARY, num_topics=NUM_TOPICS, chunksize=CHUNKSIZE, passes=PASSES, random_state=RANDOM_STATE):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        chunksize=chunksize,\n",
    "        passes=passes,\n",
    "        random_state=random_state)\n",
    "    return lda_model\n",
    "\n",
    "# Coherence (c_v: using normalized pointwise mutual information (NPMI) and the cosine similarity)\n",
    "COHERENCE = 'c_v'\n",
    "\n",
    "# Compute Coherence Score\n",
    "def compute_coherence_score(lda_model, trigram_terms, dictionary=DICTIONARY, coherence=COHERENCE):\n",
    "    coherence_model = gensim.models.CoherenceModel(model=lda_model, texts=trigram_terms, dictionary=dictionary, coherence=COHERENCE)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "\n",
    "# Optimize Number of Topics\n",
    "def optimize_num_topics(trigram_terms, corpus, dictionary=DICTIONARY):\n",
    "    lda_models = []\n",
    "    coherence_scores = []\n",
    "    for num_topics in range(NUMS_TOPICS['min'], NUMS_TOPICS['max']+1):\n",
    "        lda_model = build_lda_model(corpus, dictionary, num_topics)\n",
    "        lda_models.append(lda_model)\n",
    "        coherence_score = compute_coherence_score(lda_model, trigram_terms, dictionary)\n",
    "        coherence_scores.append(coherence_score)\n",
    "    return lda_models, coherence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attractive-truck",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# 5. Topic Model Evaluation\n",
    "###################################\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "display(HTML(\"<style>.output_area { max-width:100% !important; }</style>\"))\n",
    "\n",
    "# Save LDA Model for the year\n",
    "def save_model(year):\n",
    "    MODEL_PATH = 'Model' # Model Folder's Name\n",
    "    os.makedirs(MODEL_PATH, exist_ok=True) # Make Model Folder, if it doesn't exist\n",
    "    assert os.path.isdir(MODEL_PATH)\n",
    "    MODEL_FILE = 'topic_model_' + str(year) + '.pkl'\n",
    "    lda_model.save('%s/%s' % (MODEL_PATH, MODEL_FILE)) # to load: gensim.models.Phrases.load('%s/%s' % (MODEL_PATH, MODEL_FILE))\n",
    "\n",
    "def get_mvps(year):\n",
    "    vocab = []\n",
    "    mvps_year = mvps[mvps['year']==year].iloc[:, [1,2,3,4]]\n",
    "    names = mvps_year['name'].str.lower()\n",
    "    for name in names:\n",
    "        vocab.append(create_vocabulary([name])[0])\n",
    "    return mvps_year, vocab\n",
    "\n",
    "def print_mvps_topics(lda_model, names):\n",
    "    # the most contributing terms for the optimal number of topics\n",
    "    topics = lda_model.print_topics(num_words=300) # default=30\n",
    "    \n",
    "    for name in names:\n",
    "        full_name = '_'.join(name)\n",
    "        last_name = name[1]\n",
    "        \n",
    "        for topic in topics:\n",
    "            text = topic[1]\n",
    "            index_start = text.find(full_name) # find name by 2-gram (e.g. john_smith)\n",
    "            if index_start < 0:\n",
    "                index_start = text.find(last_name) # find name by last_name (e.g. smith)\n",
    "            if index_start > -1:\n",
    "                index_end = index_start + len(full_name) + 1\n",
    "                text_before = text[:index_start]\n",
    "                rank = text_before.count('+') + 1\n",
    "                \n",
    "                # the Top 5 Terms (abbreviate the rest of the terms)\n",
    "                i = 0\n",
    "                idx = text_before.find('+')\n",
    "                while idx > -1 and i < 4:\n",
    "                    idx = text_before.find('+', idx + 1)\n",
    "                    i += 1\n",
    "                \n",
    "                display(Markdown(f'**{name}**: {text_before[:idx]} ... **<font color=red>{text[index_start:index_end]}</font>** (**{rank}** th) ...'))\n",
    "    \n",
    "def evaluate_model(year, to_save=False):\n",
    "    # Documents for the year\n",
    "    docs = articles[articles['year']==year]['text']\n",
    "\n",
    "    trigram_terms, corpus = create_trigram_corpus(docs)\n",
    "    lda_models, coherence_scores = optimize_num_topics(trigram_terms, corpus)\n",
    "    \n",
    "    num_topics_index = coherence_scores.index(max(coherence_scores))\n",
    "    x = range(NUMS_TOPICS['min'], NUMS_TOPICS['max']+1)\n",
    "    \n",
    "    # Number of Topics Graph\n",
    "    plt.plot(x, coherence_scores)\n",
    "    plt.xticks([i for i in range(min(x), max(x)+1)])\n",
    "    plt.title(\"Number of Topics v.s. Coherence Score\")\n",
    "    plt.xlabel(\"Number of Topics\")\n",
    "    plt.ylabel(\"Coherence Score\")\n",
    "    plt.legend((\"coherence_scores\"), loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    # Optimal LDA Model\n",
    "    lda_model = lda_models[num_topics_index]\n",
    "    if to_save:\n",
    "        save_model(year)\n",
    "\n",
    "    num_topics = x[num_topics_index]\n",
    "    print('Optimal Number of Topics: ' + str(num_topics))\n",
    "    print('Best Coherence Score: ' + str(coherence_scores[num_topics_index]))\n",
    "    print('MVPs in any of the ' + str(num_topics) + ' Topics:')\n",
    "    \n",
    "    # MVPs for the year\n",
    "    mvps_year, names = get_mvps(year)\n",
    "    print(mvps_year, '\\n')\n",
    "    \n",
    "    # Print MVPs mentioned in any Topic\n",
    "    print_mvps_topics(lda_model, names)\n",
    "    \n",
    "    return lda_model, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46029258",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 6. Topic Model Visualization\n",
    "###################################\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Remove Scrollbars and boxes\n",
    "display(HTML(\"<style>\" +\n",
    "             \"div.output_scroll { height: 100% !important; box-shadow: none; }\" +\n",
    "             \"div.output_subarea { overflow-x: hidden !important; }\" +\n",
    "             \".jp-CodeCell.jp-mod-outputsScrolled .jp-Cell-outputArea { overflow-y: hidden !important; max-height: 100%; box-shadow: none; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-latitude",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# 6-1. Topic Model Year 2018\n",
    "###################################\n",
    "lda_model_2018, corpus_2018 = evaluate_model(YEARS[0]) # 2018\n",
    "pyLDAvis.gensim_models.prepare(lda_model_2018, corpus_2018, dictionary=DICTIONARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-wrist",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# 6-2. Topic Model Year 2019\n",
    "###################################\n",
    "lda_model_2019, corpus_2019 = evaluate_model(YEARS[1]) # 2019\n",
    "pyLDAvis.gensim_models.prepare(lda_model_2019, corpus_2019, dictionary=DICTIONARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-dynamics",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# 6-3. Topic Model Year 2020\n",
    "###################################\n",
    "lda_model_2020, corpus_2020 = evaluate_model(YEARS[2]) # 2020\n",
    "pyLDAvis.gensim_models.prepare(lda_model_2020, corpus_2020, dictionary=DICTIONARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-cyprus",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# 6-4. Topic Model Year 2021\n",
    "###################################\n",
    "lda_model_2021, corpus_2021 = evaluate_model(YEARS[3]) # 2021\n",
    "pyLDAvis.gensim_models.prepare(lda_model_2021, corpus_2021, dictionary=DICTIONARY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94778749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# 7. LDA Model Diagram\n",
    "###################################\n",
    "# CSS for LDA Diagram\n",
    "display(HTML(\"<style>\" +\n",
    "             \".diagram_div { text-align: center; font-size: 24px; }\" +\n",
    "             \".grand_div, .parent_div, .child_title_div, .child_doc_div, .child_text_div, .child_sentence_div, .child_box_div { padding: 10px; }\" +\n",
    "             \".grand_grand_box_div, .child_box_div { border-style: solid; border-width:thin; border-color: #00000; }\" +\n",
    "             \".grand_box_div { border-left: solid; border-right: solid; border-width:thin; border-color: #00000; }\" +\n",
    "             \".grand_grand_div, .grand_grand_box_div { display: flex; }\" +\n",
    "             \".grand_div, .grand_box_div { flex: 1; margin: auto; text-align: left; }\" +\n",
    "             \".parent_title_div { text-align: center; font-size: 16px; }\" +\n",
    "             \".parent_doc_div { width: 800px; background-color: #EAECEE; font-size: 16px; }\" +\n",
    "             \".parent_div { display: flex; width: 800px; }\" +\n",
    "             \".child_text_div, .child_sentence_div { float:left; margin: auto; text-align: center; }\" +\n",
    "             \".child_text_div { width: 100px; }\" +\n",
    "             \".child_sentence_div { width: 300px; }\" +\n",
    "             \".child_box_div { float: left; width: 200px; }</style>\"))\n",
    "\n",
    "TOPIC_COLORS = ['Crimson', 'HotPink', 'DarkOrange', 'LightSeaGreen', 'DodgerBlue', 'BlueViolet', 'Orchid', 'SeaGreen', 'MediumBlue', 'MediumSlateBlue']\n",
    "\n",
    "# Get Topic Distribution in a Document\n",
    "def get_topic_distribution(lda_model, bow):\n",
    "    topic_ids_colors = {}\n",
    "    text_doc_topics = []\n",
    "    doc_topics = lda_model.get_document_topics(bow, minimum_probability=1e-2) # 1e-2 = 10^-2 = 0.01\n",
    "    doc_topics.sort(key=lambda x:x[1], reverse=True)\n",
    "    for i, (topic_id, prob) in enumerate(doc_topics):\n",
    "        topic_ids_colors[topic_id] = TOPIC_COLORS[i]\n",
    "    return topic_ids_colors, doc_topics\n",
    "\n",
    "# Create Term with most likely Topic Map\n",
    "def create_term_topic_map(lda_model, bow):\n",
    "    terms_topic = {}\n",
    "    for word_id, freq in bow:\n",
    "        term_topics = lda_model.get_term_topics(word_id, minimum_probability=1e-5) # 1e-5 = 10^-5 = 0.00001\n",
    "        if term_topics:\n",
    "            term_topics.sort(key=lambda x:x[1], reverse=True)\n",
    "            terms_topic[DICTIONARY[word_id]] = term_topics[0] # terms_topic[term] = (topic_id, prob) <- the top tuple has the highest prob\n",
    "    return terms_topic\n",
    "\n",
    "# Display Term with most likely Topic\n",
    "def display_terms_topic(doc, terms_topic, topic_ids_colors, doc_topics):\n",
    "    text_terms_topic = []\n",
    "    text_topic_terms_probs = {}\n",
    "    for word in doc.split():\n",
    "        word_color = word\n",
    "        vocab = create_vocabulary([word])[0]; # [['shohei']] -> ['shohei']\n",
    "        if vocab and vocab[0] in terms_topic: # ['shohei'] -> 'shohei'\n",
    "            term = vocab[0]\n",
    "            if term in terms_topic:\n",
    "                topic_id = terms_topic[term][0]\n",
    "                prob = terms_topic[term][1]\n",
    "                if topic_id in topic_ids_colors:\n",
    "                    color = topic_ids_colors[topic_id]\n",
    "                    word_color = '<font color=' + color + '><b>' + word + '</b></font>'\n",
    "                    word_prob = '<font color=' + color + '><b>' + word + '</b> (%1.5f' %(prob) + ')</font><br>'\n",
    "                    text_topic_terms_probs[topic_id] = text_topic_terms_probs.get(topic_id, '') + word_prob\n",
    "        text_terms_topic.append(word_color)\n",
    "    \n",
    "    # Title Section\n",
    "    title1 = \"<b>m</b> Words Probability Distribution<br>(from Dirichlet Dist.)<br>on <b>i</b>-th Topic θi\"\n",
    "    title2 = \"<u>Document</u>: \" + \" \".join(text_terms_topic)\n",
    "    title3 = \"<b>k</b> Topics Probability Distribution<br>(from Dirichlet Dist.)<br>on <b>d</b>-th Document πd\"\n",
    "    titles = \"<div class='grand_grand_div'>\"\n",
    "    titles += \"<div class='grand_div'><div class='parent_title_div'>\" + title1 + \"</div></div>\"\n",
    "    titles += \"<div class='grand_div'><div class='parent_doc_div'>\" + title2 + \"</div></div>\"\n",
    "    titles += \"<div class='grand_div'><div class='parent_title_div'>\" + title3 + \"</div></div></div>\"\n",
    "\n",
    "    # Diagram Section\n",
    "    html = ''\n",
    "    for i, (topic_id, prob) in enumerate(doc_topics):\n",
    "        words_probs = text_topic_terms_probs[topic_id]\n",
    "        color = topic_ids_colors[topic_id]\n",
    "        id = \"<font color='\" + color + \"'><b>\" + str(topic_id) + \"</b></font>\"\n",
    "        words = \"p(w1|θ\" + id + \")<br>p(w2|θ\" + id + \")<br>...<br>p(wm|θ\" + id + \")\"\n",
    "        topic = \"← Topic θ\" + id\n",
    "        formula = \"← p(θ\" + id + \") = p(πd,\" + id + \") = <u>Topic \" + id + \": \" + str(prob) + \"</u> ←\"\n",
    "        html += \"<div class='parent_div'><div class='child_text_div'>\" + words + \"</div><div class='child_text_div'>\" + topic + \"</div><div class='child_box_div'>\" + words_probs + \"</div><div class='child_sentence_div'>\" + formula + \"</div></div>\" # color = topic_ids_colors[topic_id]\n",
    "\n",
    "    nbsps = \"&nbsp;&nbsp;&nbsp;&nbsp;\"\n",
    "    div1 = nbsps + \"→<br>p(W|θ<b>i</b>)<br>= (p(w1|θi),<br>\" + nbsps + \"p(w2|θi)),<br>\" + nbsps + \"...<br>\" + nbsps + \"p(w<b>m</b>|θi))<br><br><br>\"\n",
    "    div1 += \"= (Dirichlet(β1),<br>\" + nbsps + \"Dirichlet(β2),<br>\" + nbsps + \"...,<br>\" + nbsps + \"Dirichlet(β<b>m</b>))<br>\"\n",
    "    div2 = nbsps + \"→<br>p(π<b>d</b>)<br>= (p(πd,1),<br>\" + nbsps + \"p(πd,2),<br>\" + nbsps + \"...<br>\" + nbsps + \"p(πd,<b>k</b>))<br><br><br>\"\n",
    "    div2 += \"= (Dirichlet(α1),<br>\" + nbsps + \"Dirichlet(α2),<br>\" + nbsps + \"...,<br>\" + nbsps + \"Dirichlet(α<b>k</b>))<br><br><br>\"\n",
    "    div2 += \"=<br>p(θ)<br>= (p(θ1),<br>\" + nbsps + \"p(θ2),<br>\" + nbsps + \"...<br>\" + nbsps + \"p(θ<b>k</b>))<br>\"\n",
    "    divs = \"<div class='grand_grand_box_div'>\"\n",
    "    divs += \"<div class='grand_div'>\" + div1 + \"</div>\"\n",
    "    divs += \"<div class='grand_box_div'>\" + html + \"</div>\"\n",
    "    divs += \"<div class='grand_div'>\" + div2 + \"</div></div>\"\n",
    "    display(HTML(titles + divs))\n",
    "    display(Markdown(f'------------------------------------------------------------------------------------------------------------------------'))\n",
    "\n",
    "# Display Diagram\n",
    "def display_diagram(lda_model, docs):\n",
    "    trigram_terms_2021, corpus_2021 = create_trigram_corpus(docs)\n",
    "    for i, bow in enumerate(corpus_2021):\n",
    "        \n",
    "        # Get Topic Distribution in a Document\n",
    "        topic_ids_colors, doc_topics = get_topic_distribution(lda_model, bow)\n",
    "        \n",
    "        # Create Term with most likely Topic Map\n",
    "        terms_topic = create_term_topic_map(lda_model, bow)\n",
    "        \n",
    "        # Display Term with most likely Topic\n",
    "        display_terms_topic(docs[i], terms_topic, topic_ids_colors, doc_topics)\n",
    "\n",
    "# LDA Model Diagram\n",
    "display(HTML(\"<div class='diagram_div'><u>LDA Model Diagram: m Words per Topic & k Topics per Document - 2 samples</u></div>\"))\n",
    "docs_2021 = [articles[articles['id']==\"20210514403\"].text.values[0], articles[articles['id']==\"20210517104\"].text.values[0]] # ['shohei ohtani ...', '']\n",
    "display_diagram(lda_model_2021, docs_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c6f466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlb",
   "language": "python",
   "name": "mlb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
